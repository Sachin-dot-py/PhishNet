from flask import Flask, request, jsonify
import os
import requests
import random
import uuid
from dotenv import load_dotenv
from pymongo import MongoClient, UpdateOne
from email_malicious import predict_malicious_email
from tweet_malicious import predict_malicious_tweet
from generate_text import generate_email, generate_tweet, generate_message

load_dotenv()
app = Flask(__name__)
client = MongoClient(os.environ.get("MONGO_URI"))
db = client.PhishNet
questions_collection = db.questions

ACCOUNT_ID = os.environ.get("CLOUDFLARE_ACCOUNT_ID")
AUTH_TOKEN = os.environ.get("CLOUDFLARE_AUTH_TOKEN")
LLM_API_URL = f"https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/@cf/google/gemma-2b-it-lora"

@app.route('/api/game/question/starter', methods=['GET'])
def get_multiple_questions():
    """
    Returns a list of three random questions.
    JSON Response Format:
    [
        {
            "id": str,  # A unique identifier for the question
            "type": str,
            "content": str (if type is "message" or "tweet"),
            "subject": str (if type is "email"),
            "body": str (if type is "email"),
            "malicious": bool  # Whether the text/tweet/email is malicious
        },
        ...
    ]
    """
    types = random.choices(["message", "tweet", "email"], k=3)
    questions = []
    for question_type in types:
        isMalicious = random.choice([True, False])
        new_id = str(uuid.uuid4())
        if question_type == "message":
            content = generate_message(isMalicious)
            questions.append({"id": new_id, "type": question_type, "content": content, "malicious": isMalicious})
        elif question_type == "tweet":
            content = generate_tweet(isMalicious)
            questions.append({"id": new_id, "type": question_type, "content": content, "malicious": isMalicious})
        elif question_type == "email":
            subject, body = generate_email(isMalicious)
            questions.append({"id": new_id, "type": question_type, "subject": subject, "body": body, "malicious": isMalicious})

    bulk_ops = [
        UpdateOne(
            {"id": question["id"]},
            {"$set": question},
            upsert=True
        )
        for question in questions
    ]
    
    if bulk_ops:
        questions_collection.bulk_write(bulk_ops)
    
    return jsonify(questions)

@app.route('/api/game/question/lazy_loading', methods=['GET'])
def get_single_question():
    """
    Returns a list containing a single random question.
    JSON Response Format:
    [
        {
            "id": str,  # Unique identifier for the question
            "type": str,  # One of "message", "tweet", or "email"
            "content": str (if type is "message" or "tweet"), # Max 100 words
            "subject": str (if type is "email"), # Max 15 words
            "body": str (if type is "email") # Max 100 words,
            "malicious": bool  # Whether the text/tweet/email is malicious
        }
    ]
    """
    type_ = random.choice(["message", "tweet", "email"])
    isMalicious = random.choice([True, False])
    new_id = str(uuid.uuid4())
    if type_ == "message":
        content = generate_message(isMalicious)
        question = {"id": new_id, "type": type_, "content": content, "malicious": isMalicious}
    elif type_ == "tweet":
        content = generate_tweet(isMalicious)
        question = {"id": new_id, "type": type_, "content": content, "malicious": isMalicious}
    elif type_ == "email":
        subject, body = generate_email(isMalicious)
        question = {"id": new_id, "type": type_, "subject": subject, "body": body, "malicious": isMalicious}
    
    questions_collection.update_one(
        {"id": question["id"]},
        {"$set": question},
        upsert=True
    )
    return jsonify([question])

@app.route('/api/game/getFeedback', methods=['POST'])
def post_feedback():
    """
    Receives user feedback on whether an email, tweet, or message is malicious.
    
    Expected JSON Request for email:
    {
        "id": str,
        "type": "email",
        "userMalicious": bool,
        "wordsListSubject": list,  # Indices of red flag words in the subject
        "wordsListBody": list      # Indices of red flag words in the body
    }
    
    Expected JSON Request for tweet or message:
    {
        "id": str,
        "type": "tweet" or "message",
        "userMalicious": bool,
        "wordsList": list         # Indices of red flag words in the content
    }
    
    JSON Response:
    {
        "feedback": str,   # Feedback message generated by the LLM
        "correct": bool    # Whether the userâ€™s guess was correct
    }
    """
    data = request.get_json()
    item_id = data.get("id")
    item = questions_collection.find_one({"id": item_id})
    item_type = item.get("type")
    userMalicious = data.get("userMalicious")

    # Helper function for ordinal representation.
    def ordinal(n):
        if 10 <= n % 100 <= 20:
            suffix = 'th'
        else:
            suffix = {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th')
        return f"{n}{suffix}"
    
    # Initialize variables that will be used later.
    prompt = ""
    malicious = None  # ground truth malicious flag
    advisor = ""
    
    if item_type == "email":
        # Retrieve email details (stored in questions_collection)
        if not item:
            return jsonify({"error": "Invalid email ID"}), 400

        malicious = item["malicious"]
        if userMalicious == malicious:
            return jsonify({"feedback": "Correct! Well done.", "correct": True})
        
        # Process red flag words for email subject and body.
        wordsListSubject = data.get("wordsListSubject", [])
        wordsListBody = data.get("wordsListBody", [])
        
        subject_words = item.get('subject', '').split()
        body_words = item.get('body', '').split()
        
        selected_subject_words = []
        for idx in wordsListSubject:
            position = ordinal(idx + 1)
            word = subject_words[idx] if idx < len(subject_words) else "<invalid index>"
            selected_subject_words.append(f"the {position} \"{word}\"")
        
        selected_body_words = []
        for idx in wordsListBody:
            position = ordinal(idx + 1)
            word = body_words[idx] if idx < len(body_words) else "<invalid index>"
            selected_body_words.append(f"the {position} \"{word}\"")
        
        if selected_subject_words and selected_body_words:
            selected_words_description = (
                "Subject: " + ", ".join(selected_subject_words) + 
                "; Body: " + ", ".join(selected_body_words)
            )
        elif selected_subject_words:
            selected_words_description = "Subject: " + ", ".join(selected_subject_words)
        elif selected_body_words:
            selected_words_description = "Body: " + ", ".join(selected_body_words)
        else:
            selected_words_description = "None"
        
        prompt = f"""
        Email Details:
        ---------------
        Subject: {item.get('subject', '')}
        Body: {item.get('body', '')}
        Malicious (ground truth): {item.get('malicious')}
        User's Guess: {userMalicious}

        The user selected the following words as red flags:
        {selected_words_description}

        Based on the email content and the user's selections, provide personalized feedback that explains:
        - Why their guess might be incorrect (if it is),
        - What red flag words in the email contributed to making it suspicious,
        and any brief insights that could help the user learn.
        Your response should ONLY include the feedback message to be shown to the user. Address the user directly.
        """
        advisor = "email security advisor"
    
    elif item_type in ["tweet", "message"]:
        # Determine which collection and advisor to use.
        if item_type == "tweet":
            advisor = "social media security advisor"
            label = "Tweet"
        else:  # message
            advisor = "messaging security advisor"
            label = "Message"
        
        if not item:
            return jsonify({"error": f"Invalid {item_type} ID"}), 400

        malicious = item["malicious"]
        if userMalicious == malicious:
            return jsonify({"feedback": "Correct! Well done.", "correct": True})
        
        wordsList = data.get("wordsList", [])
        content_words = item.get('content', '').split()
        
        selected_words = []
        for idx in wordsList:
            position = ordinal(idx + 1)
            word = content_words[idx] if idx < len(content_words) else "<invalid index>"
            selected_words.append(f"the {position} \"{word}\"")
        
        selected_words_description = "Content: " + ", ".join(selected_words) if selected_words else "None"
        
        prompt = f"""
        {label} Details:
        --------------
        Content: {item.get('content', '')}
        Malicious (ground truth): {item.get('malicious')}
        User's Guess: {userMalicious}

        The user selected the following words as red flags:
        {selected_words_description}

        Based on the {label.lower()} content and the user's selections, provide personalized feedback that explains:
        - Why their guess might be incorrect (if it is),
        - What red flag words in the {label.lower()} contributed to making it suspicious,
        and any brief insights that could help the user learn.
        Your response should ONLY include the feedback message to be shown to the user.
        """
    
    else:
        return jsonify({"error": "Invalid type. Must be 'email', 'tweet', or 'message'."}), 400

    # Send the prompt to the LLM for generating feedback.
    response = requests.post(
        LLM_API_URL,
        headers={"Authorization": f"Bearer {AUTH_TOKEN}"},
        json={
            "messages": [
                {"role": "system", "content": f"You are an expert {advisor}."},
                {"role": "user", "content": prompt}
            ]
        }
    )
    
    feedback = response.json().get("result", {}).get("response", "")
    
    return jsonify({
        "feedback": feedback,
        "correct": (malicious == userMalicious)
    })

def get_email_explanation(score, subject, body):
    if score > 0.7:
        prompt = (
            "Analyze the following email subject and body. "
            "Based on research, emails are often malicious if they exhibit urgent language, mismatched sender details, "
            "suspicious links/attachments, grammatical errors, or unusual requests for sensitive info. "
            "Provide a single, concise explanation (maximum 25 words) stating exactly which red flag(s) triggered the malicious score. "
            "Output only the explanation text. "
            f"Subject: {subject} Body: {body}"
        )
    else:
        prompt = (
            "Analyze the following email subject and body. "
            "Based on research, emails are considered safe when they lack urgent language, mismatched sender details, "
            "suspicious links/attachments, grammatical errors, or unusual requests for sensitive info. "
            "Provide a single, concise explanation (maximum 25 words) stating why the email appears safe. "
            "Output only the explanation text. "
            f"Subject: {subject} Body: {body}"
        )
    
    response = requests.post(
        LLM_API_URL,
        headers={"Authorization": f"Bearer {AUTH_TOKEN}"},
        json={
            "messages": [
                {"role": "system", "content": "You are an expert security assistant. You only have one chance to analyze the email and are only allowed to provide explanation text according to the specified requirements, with no context or follow up questions permited."},
                {"role": "user", "content": prompt}
            ]
        }
    )
    result = response.json()
    explanation = result.get("result").get("response")
    return explanation

def get_tweet_explanation(score, content):
    if score > 0.7:
        prompt = (
            "Analyze the following tweet content. "
            "Based on research, tweets are often malicious if they sound too good to be true, exhibit urgent language, deal with money or prizes, "
            "suspicious links, grammatical errors, or unusual requests for sensitive info. "
            "Provide a single, concise explanation (maximum 25 words) stating exactly which red flag(s) triggered the malicious score. "
            "Output only the explanation text. "
            f"Tweet Content: {content}"
        )
    else:
        prompt = (
            "Analyze the following tweet content. "
            "Based on research, tweets are considered safe when they sound reasonable, lack urgent language, "
            "suspicious links, grammatical errors, or unusual requests for sensitive info. "
            "Provide a single, concise explanation (maximum 25 words) stating why the tweet appears safe. "
            "Output only the explanation text. "
            f"Tweet Content: {content}"
        )
    
    response = requests.post(
        LLM_API_URL,
        headers={"Authorization": f"Bearer {AUTH_TOKEN}"},
        json={
            "messages": [
                {"role": "system", "content": "You are an expert security assistant. You only have one chance to analyze the tweet and are only allowed to provide explanation text according to the specified requirements, with no context or follow up questions permited."},
                {"role": "user", "content": prompt}
            ]
        }
    )
    result = response.json()
    explanation = result.get("result").get("response")
    return explanation

@app.route('/api/blocker/predictMalicious', methods=['POST'])
def predict_malicious():
    """
    Takes in a JSON payload with the following format:
    {
        "type": str,         # One of "tweet" or "email"
        "content": str,      # Provided if type is "tweet" 
        "subject": str,      # Provided if type is "email"
        "body": str          # Provided if type is "email"
    }

    Returns a JSON response in the following format:
    {
        "score": float,         # A float value between 0 and 1 representing the probability of the content being malicious
        "explanation": str      # Explanation of why we think it's malicious or not, for display to the user
    }
    """
    data = request.get_json()

    if data.get("type") not in ["tweet", "email"]:
        return jsonify({"error": "Invalid content type"}), 400
    
    if data.get("type") == "email":
        score = predict_malicious_email(data.get("subject", ""), data.get("body", ""))
        explanation = get_email_explanation(score, data.get("subject", ""), data.get("body", ""))
    elif data.get("type") == "tweet":
        score = predict_malicious_tweet(data.get("content", ""))
        explanation = get_tweet_explanation(score, data.get("content", ""))

    return jsonify({
        "score": score,
        "explanation": explanation
    })

if __name__ == '__main__':
    app.run(debug=True)